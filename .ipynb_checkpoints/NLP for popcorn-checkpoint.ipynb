{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd = 'C:\\\\github\\\\NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(pwd, 'data', 'popcorn/labeledTrainData.tsv'),\n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(os.path.join(pwd, 'data', 'popcorn/testData.tsv'),\n",
    "                   header=0, delimiter=\"\\t\",\n",
    "                   quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head(5)\n",
    "\n",
    "\n",
    "def preprocessing(review, remove_stopwords=False):\n",
    "\n",
    "    review_text = BeautifulSoup(review, 'lxml').get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return words\n",
    "\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(preprocessing(review, True))\n",
    "\n",
    "clean_train_df = pd.DataFrame({'review': clean_train_reviews, 'sentiment':train['sentiment']})\n",
    "\n",
    "tokenizer.fit_on_texts(clean_train_reviews)\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
    "\n",
    "#print(text_sequences[0])\n",
    "\n",
    "word_vocab = tokenizer.word_index\n",
    "#print(word_vocab)\n",
    "\n",
    "#len(word_vocab)\n",
    "\n",
    "data_configs = {}\n",
    "data_configs['vocab'] = word_vocab\n",
    "data_configs['vocab_size'] = len(word_vocab)+1\n",
    "\n",
    "#data_configs\n",
    "\n",
    "max_len = 174\n",
    "train_inputs = pad_sequences(text_sequences,maxlen=max_len,padding='post')\n",
    "#print(train_inputs.shape)\n",
    "\n",
    "train_labels = np.array(train['sentiment'])\n",
    "#train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN_PATH = './data_in/'\n",
    "TRAIN_INPUT_DATA = 'train_input.npy'\n",
    "TRAIN_LABEL_DATA = 'train_label.npy'\n",
    "TRAIN_CLEAN_DATA = 'train_clean.csv'\n",
    "DATA_CONFIGS = 'data_configs.json'\n",
    "\n",
    "if not os.path.exists(DATA_IN_PATH):\n",
    "     os.makedirs(DATA_IN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)\n",
    "np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)\n",
    "\n",
    "clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)\n",
    "json.dump(data_configs,open(DATA_IN_PATH + DATA_CONFIGS,'w'),ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"./data/popcorn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./data/popcorn/testData.tsv\", header=0, delimiter='\\t', quoting=3)\n",
    "clean_test_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in test_data['review']:\n",
    "    clean_test_reviews.append(preprocessing(review,remove_stopwords=True))\n",
    "    \n",
    "clean_test_df = pd.DataFrame({'review': clean_test_reviews, 'id':test['id']})\n",
    "test_id = np.array(test_data['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(clean_test_reviews)\n",
    "text_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\n",
    "text_inputs = pad_sequences(text_sequences, maxlen = max_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = text_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INPUT_DATA = 'test_input.npy'\n",
    "TEST_CLEAN_DATA = 'test_clean.csv'\n",
    "TEST_ID_DATA = 'test_id.npy'\n",
    "\n",
    "np.save(open(datapath + TEST_INPUT_DATA, 'wb'), test_inputs)\n",
    "np.save(open(datapath + TEST_ID_DATA, 'wb'), test_id)\n",
    "\n",
    "clean_test_df.to_csv(datapath + TEST_CLEAN_DATA, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT_PATH = './data_out/'\n",
    "\n",
    "#train_data =train\n",
    "reviews = list(train['review'])\n",
    "sentiments = list(train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True,ngram_range=(1,3),max_features=5000)\n",
    "\n",
    "X = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 23343931 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rseed = 52\n",
    "TEST_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(sentiments)\n",
    "X_train, X_eval,y_train,y_eval = train_test_split(X,y,test_size=TEST_SPLIT,random_state=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs = LogisticRegression(class_weight = 'balanced')\n",
    "lgs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs.score(X_eval,y_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                   level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 19\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-10 17:49:12,755 : INFO : collecting all words and their counts\n",
      "2020-08-10 17:49:12,755 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-10 17:49:13,781 : INFO : PROGRESS: at sentence #10000, processed 2354780 words, keeping 163178 word types\n",
      "2020-08-10 17:49:14,886 : INFO : PROGRESS: at sentence #20000, processed 4686268 words, keeping 251892 word types\n",
      "2020-08-10 17:49:15,313 : INFO : collected 289705 word types from a corpus of 5844706 raw words and 25000 sentences\n",
      "2020-08-10 17:49:15,313 : INFO : Loading a fresh vocabulary\n",
      "2020-08-10 17:49:15,521 : INFO : effective_min_count=40 retains 9563 unique words (3% of original 289705, drops 280142)\n",
      "2020-08-10 17:49:15,521 : INFO : effective_min_count=40 leaves 5008901 word corpus (85% of original 5844706, drops 835805)\n",
      "2020-08-10 17:49:15,580 : INFO : deleting the raw counts dictionary of 289705 items\n",
      "2020-08-10 17:49:15,588 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2020-08-10 17:49:15,588 : INFO : downsampling leaves estimated 3711134 word corpus (74.1% of prior 5008901)\n",
      "2020-08-10 17:49:15,650 : INFO : estimated required memory for 9563 words and 300 dimensions: 27732700 bytes\n",
      "2020-08-10 17:49:15,650 : INFO : resetting layer weights\n",
      "2020-08-10 17:49:19,018 : INFO : training model with 4 workers on 9563 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=19\n",
      "2020-08-10 17:49:20,045 : INFO : EPOCH 1 - PROGRESS: at 10.00% examples, 365789 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:49:21,049 : INFO : EPOCH 1 - PROGRESS: at 20.62% examples, 384189 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:22,067 : INFO : EPOCH 1 - PROGRESS: at 31.46% examples, 386575 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:49:23,071 : INFO : EPOCH 1 - PROGRESS: at 41.84% examples, 386523 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:24,074 : INFO : EPOCH 1 - PROGRESS: at 52.44% examples, 388537 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:25,086 : INFO : EPOCH 1 - PROGRESS: at 63.08% examples, 388658 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:49:26,109 : INFO : EPOCH 1 - PROGRESS: at 72.69% examples, 382731 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:27,134 : INFO : EPOCH 1 - PROGRESS: at 83.00% examples, 381228 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:28,140 : INFO : EPOCH 1 - PROGRESS: at 92.69% examples, 378206 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:28,796 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-10 17:49:28,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-10 17:49:28,842 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-10 17:49:28,846 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-10 17:49:28,847 : INFO : EPOCH - 1 : training on 5844706 raw words (3711645 effective words) took 9.8s, 377890 effective words/s\n",
      "2020-08-10 17:49:29,859 : INFO : EPOCH 2 - PROGRESS: at 8.87% examples, 328893 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:49:30,864 : INFO : EPOCH 2 - PROGRESS: at 19.35% examples, 361655 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:31,873 : INFO : EPOCH 2 - PROGRESS: at 29.68% examples, 369155 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:32,876 : INFO : EPOCH 2 - PROGRESS: at 40.01% examples, 371778 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:33,885 : INFO : EPOCH 2 - PROGRESS: at 50.47% examples, 375042 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:34,893 : INFO : EPOCH 2 - PROGRESS: at 61.03% examples, 377761 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:35,904 : INFO : EPOCH 2 - PROGRESS: at 71.62% examples, 379422 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:36,916 : INFO : EPOCH 2 - PROGRESS: at 82.34% examples, 380479 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:49:37,914 : INFO : EPOCH 2 - PROGRESS: at 92.69% examples, 380397 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:38,526 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-10 17:49:38,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-10 17:49:38,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-10 17:49:38,569 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-10 17:49:38,569 : INFO : EPOCH - 2 : training on 5844706 raw words (3711861 effective words) took 9.7s, 382039 effective words/s\n",
      "2020-08-10 17:49:39,584 : INFO : EPOCH 3 - PROGRESS: at 10.36% examples, 382722 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:40,620 : INFO : EPOCH 3 - PROGRESS: at 20.97% examples, 386747 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:49:41,622 : INFO : EPOCH 3 - PROGRESS: at 31.46% examples, 386038 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:42,637 : INFO : EPOCH 3 - PROGRESS: at 42.18% examples, 387768 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:49:43,647 : INFO : EPOCH 3 - PROGRESS: at 52.72% examples, 389008 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:44,649 : INFO : EPOCH 3 - PROGRESS: at 63.08% examples, 387516 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-10 17:49:45,676 : INFO : EPOCH 3 - PROGRESS: at 73.83% examples, 387906 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:46,684 : INFO : EPOCH 3 - PROGRESS: at 85.18% examples, 391207 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:47,690 : INFO : EPOCH 3 - PROGRESS: at 95.94% examples, 391095 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:48,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-10 17:49:48,037 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-10 17:49:48,037 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-10 17:49:48,042 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-10 17:49:48,044 : INFO : EPOCH - 3 : training on 5844706 raw words (3709679 effective words) took 9.5s, 391859 effective words/s\n",
      "2020-08-10 17:49:49,050 : INFO : EPOCH 4 - PROGRESS: at 9.82% examples, 367887 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:50,076 : INFO : EPOCH 4 - PROGRESS: at 20.32% examples, 377516 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:51,080 : INFO : EPOCH 4 - PROGRESS: at 30.84% examples, 382151 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-10 17:49:52,084 : INFO : EPOCH 4 - PROGRESS: at 41.64% examples, 386189 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:53,100 : INFO : EPOCH 4 - PROGRESS: at 52.44% examples, 388681 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:54,111 : INFO : EPOCH 4 - PROGRESS: at 62.89% examples, 387533 words/s, in_qsize 6, out_qsize 1\n",
      "2020-08-10 17:49:55,131 : INFO : EPOCH 4 - PROGRESS: at 74.03% examples, 390087 words/s, in_qsize 8, out_qsize 1\n",
      "2020-08-10 17:49:56,131 : INFO : EPOCH 4 - PROGRESS: at 84.83% examples, 391064 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:57,152 : INFO : EPOCH 4 - PROGRESS: at 95.78% examples, 391182 words/s, in_qsize 8, out_qsize 2\n",
      "2020-08-10 17:49:57,476 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-10 17:49:57,480 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-10 17:49:57,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-10 17:49:57,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-10 17:49:57,501 : INFO : EPOCH - 4 : training on 5844706 raw words (3711705 effective words) took 9.5s, 392721 effective words/s\n",
      "2020-08-10 17:49:58,538 : INFO : EPOCH 5 - PROGRESS: at 10.36% examples, 375574 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:49:59,541 : INFO : EPOCH 5 - PROGRESS: at 20.80% examples, 385703 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:50:00,540 : INFO : EPOCH 5 - PROGRESS: at 31.24% examples, 385614 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:50:01,549 : INFO : EPOCH 5 - PROGRESS: at 42.01% examples, 388324 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:50:02,552 : INFO : EPOCH 5 - PROGRESS: at 52.59% examples, 390151 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:50:03,556 : INFO : EPOCH 5 - PROGRESS: at 63.23% examples, 390269 words/s, in_qsize 7, out_qsize 0\n",
      "2020-08-10 17:50:04,566 : INFO : EPOCH 5 - PROGRESS: at 73.66% examples, 389395 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:50:05,582 : INFO : EPOCH 5 - PROGRESS: at 84.46% examples, 389693 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-10 17:50:06,614 : INFO : EPOCH 5 - PROGRESS: at 95.45% examples, 389441 words/s, in_qsize 8, out_qsize 0\n",
      "2020-08-10 17:50:06,980 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-08-10 17:50:06,995 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-08-10 17:50:07,006 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-08-10 17:50:07,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-08-10 17:50:07,019 : INFO : EPOCH - 5 : training on 5844706 raw words (3710546 effective words) took 9.5s, 390212 effective words/s\n",
      "2020-08-10 17:50:07,021 : INFO : training on a 29223530 raw words (18555436 effective words) took 48.0s, 386564 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                         workers=num_workers,\n",
    "                         size=num_features,\n",
    "                         min_count=min_word_count,\n",
    "                         window=context,\n",
    "                         sample=downsampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-10 17:50:07,031 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2020-08-10 17:50:07,034 : INFO : not storing attribute vectors_norm\n",
      "2020-08-10 17:50:07,035 : INFO : not storing attribute cum_table\n",
      "2020-08-10 17:50:07,473 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words,model,num_features):\n",
    "    feature_vector = np.zeros((num_features),dtype=np.float32)\n",
    "    \n",
    "    num_words=0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            feature_vector = np.add(feature_vector,model[w])\n",
    "            \n",
    "    feature_vector=np.divide(feature_vector,num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset = list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s,model,num_features))\n",
    "        \n",
    "    reviewFeatureVecs = np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs=get_dataset(sentences,model,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data_vecs\n",
    "y = np.array(sentiments)\n",
    "X_train, X_eval,y_train,y_eval = train_test_split(X,y,test_size=TEST_SPLIT,random_state=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs = LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8358"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgs.score(X_eval,y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",max_features = 5000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, eval_input, train_label,eval_label = train_test_split(train_data_features,y,test_size=0.2,random_state=rseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest.fit(train_input,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(eval_input,eval_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_configs = None\n",
    "\n",
    "with open('./data_in/data_configs.json','r') as f:\n",
    "    prepro_configs=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping_fn(X,Y):\n",
    "    inputs,labels = {'x':X},Y\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_train,label_train))\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.repeat(count=NUM_EPOCHS)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_test,label_test))\n",
    "    dataset = dataset.map(mapping_fn)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = prepro_configs['vocab_size']\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "HIDDEN_STATE_DIM = 150\n",
    "DENSE_FEATURE_DIM = 150\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features,labels,mode):\n",
    "    TRAIN = mode ==tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL = mode = tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT = mode ==tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE,WORD_EMBEDDING_DIM)(features['x'])\n",
    "    embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)\n",
    "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM,HIDDEN_STATE_DIM]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
